from collections import Counter
from typing import Optional

def normalize (text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if yo2e == True:
        text = text.replace('Ð','Ð•').replace('Ñ‘','Ðµ')
    if casefold == True:
        text = text.casefold()
    for spaces in ['\t', '\r', '\n']:
        text = text.replace(spaces,' ')
    words = text.split()
    text = ' '.join(words)
    return text
# print (normalize("ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t"))
# print (normalize("Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°"))
# print (normalize("Hello\r\nWorld"))
# print (normalize("  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  "))

import re
def tokenize (text: str) -> list[str]:
    simv = r'\b[Ð°-ÑÑ‘a-zA-Z0-9_]+(?:-[Ð°-ÑÑ‘a-zA-Z0-9_]+)*\b'
    return re.findall(simv,text)
# print (tokenize('Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€'))
# print (tokenize('hello, word!!!'))
# print (tokenize('Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾'))
# print (tokenize('2025 Ð³Ð¾Ð´'))
# print (tokenize('emoji ðŸ˜ƒ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾'))

def count_freq(tokens: list[str], top_n: Optional[int] = None) -> dict[str, int]:
    counter = Counter(tokens)
    if top_n is not None:
        return dict(counter.most_common(top_n))
    return dict(counter)
# print (count_freq(["a","b","a","c","b","a"]))
# print (count_freq(["a","b","a","c","b","a"]))
# print (count_freq(["bb","aa","bb","aa","cc"]))